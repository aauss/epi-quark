# Algorithm agnostic evaluation for (disease) outbreak detection

## Motivation

In the field of disease outbreak detection, qualitatively different families of algorithms are used such as Farrington Flexible and SaTScan. However, comparing the performance of different algorithm families is not trivial. Inputs and outputs differ vastly differ between them and therefore make a clear quantitative comparison difficult.

Our score offers a solution to make formerly non comparable approaches comparable.

## Installation

To run the notebooks and to be able to use the `Scorer` class, you need to install the packages listed in `env.yml`. If you use conda, simply run

```
conda env create -f environment.yml
```

Afterwards, run

```
conda activate scoring
```

to activate the conda environment.

## How to use the scorer

### What do I need?

The scorer requires the following inputs:

#### **data**

This is a `pandas.DataFrame` that contains case counts of a certain infectious disease where each row aggregates the case numbers along the lowest possible resolution over some dimensions.

Such a table could look like this:

| x_week | x_county | data_label | value |
| ------ | -------- | ---------- | ----- |
| 0      | 0        | endemic    | 0     |
| 0      | 1        | endemic    | 3     |
| 1      | 0        | one        | 0     |
| 1      | 1        | one        | 1     |
| 2      | 0        | two        | 1     |
| 2      | 1        | two        | 1     |

where the data labels are extracted from the `data_label` column, its values form the `value` column, and the remaining columns are treated as coordinates.

In this table `one` and `two` are the labels for an outbreak and `endemic` are cases that don't belong to an outbreak. `non_cases` contain a 1 if a cell has no cases and 0 otherwise. This is handled internally and therefore no `data_label` should be named `non_cases`.

The coordinate system of the data DataFrame is considered complete, i.e., all relevant cells for a analysis should be found in the coordinate columns.

#### **signals**

This is also a `pandas.DataFrame` that should be a subset of the coordinates defined the `cases` DataFrame. Instead of a `data_label` column, this DataFrame should contain a `signal_label` column which contains the different signal labels generated by the outbreak detection algorithms. The significance of the signal is entered in to the `value` column. All `signal_labels` must start with a "w\_":

| x_week | x_county | signal_label | value |
| ------ | -------- | ------------ | ----- |
| 0      | 0        | w_endemic    | 0     |
| 0      | 1        | w_endemic    | 3     |
| 1      | 0        | w_A          | 0     |
| 1      | 1        | w_A          | 1     |
| 2      | 0        | w_B          | 1     |
| 2      | 1        | w_B          | 1     |

<!-- #### **Missing Signals**

THIS SECTION WILL BE USED WHEN WE DECIDE WHETHER THIS IS DONE AUTOMATICALLY OR SHOULD BE DONE MANUALLY

Finally, we might need to generate two missing signals. In the example above, we see that we only the columns `w_A` and `w_B` but not `w_endemic` and `w_non_case`. There are many such algorithms that don't specifically generate a signal for these `data_labels`. In this case, we need to estimate these signals.

This is done by assuming that if an algorithm has a certain signal $w_{i,j}$ for cell $i,j$ and there are no cases is this cell, `w_non_case` is $1-w_{i,j}$. If this cell contains cases, then `w_endemic` is $1-w_{i,j}$.

If we have several signals, we also generate many signals for `w_endemic` and `w_non_case`. In our example, we have two signals, therefore we would generate signals named `_A_endemic`, `_B_endemic`, `_A_non_case`, and `_B_non_case`. How do we aggregate these signals?

The `missing_signal_agg` allows to set the aggregation strategy. The default is "min" which would take the lowest signal for `non_case` and `endemic` respectively from all generated signals per cell. -->
